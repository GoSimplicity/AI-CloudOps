app:
  debug: true
  host: 0.0.0.0
  log_level: WARNING
  port: 8080
kubernetes:
  config_path: ./deploy/kubernetes/config
  in_cluster: false
  namespace: default
llm:
  max_tokens: 2048
  model: Qwen/Qwen3-14B
  ollama_base_url: http://127.0.0.1:11434/v1
  ollama_model: qwen2.5:3b
  provider: openai
  task_model: Qwen/Qwen2.5-14B-Instruct
  temperature: 0.7
notification:
  enabled: true
prediction:
  max_instances: 20
  min_instances: 1
  model_path: data/models/time_qps_auto_scaling_model.pkl
  prometheus_query: rate(nginx_ingress_controller_nginx_process_requests_total{service="ingress-nginx-controller-metrics"}[10m])
  scaler_path: data/models/time_qps_auto_scaling_scaler.pkl
prometheus:
  host: 127.0.0.1:9090
  timeout: 30
rag:
  cache_expiry: 3600
  chunk_overlap: 200
  chunk_size: 1000
  collection_name: aiops-assistant
  knowledge_base_path: data/knowledge_base
  max_context_length: 4000
  max_docs_per_query: 8
  ollama_embedding_model: nomic-embed-text
  openai_embedding_model: Pro/BAAI/bge-m3
  similarity_threshold: 0.7
  temperature: 0.1
  top_k: 4
  use_document_compressor: true
  use_enhanced_retrieval: true
  vector_db_path: data/vector_db
rca:
  anomaly_threshold: 0.65
  correlation_threshold: 0.7
  default_metrics:
  - container_cpu_usage_seconds_total
  - container_memory_working_set_bytes
  - kube_pod_container_status_restarts_total
  - kube_pod_status_phase
  - node_cpu_seconds_total
  - node_memory_MemFree_bytes
  - kubelet_http_requests_duration_seconds_count
  - kubelet_http_requests_duration_seconds_sum
  default_time_range: 30
  max_time_range: 1440
tavily:
  max_results: 5
testing:
  skip_llm_tests: true
