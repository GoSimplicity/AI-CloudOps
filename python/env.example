# ==============================================
# AIOps平台环境变量配置文件
# ==============================================

# 应用基础配置
DEBUG=false
HOST=0.0.0.0
PORT=8080
LOG_LEVEL=INFO

# Prometheus配置
PROMETHEUS_HOST=127.0.0.1:9090
PROMETHEUS_TIMEOUT=30

# LLM模型配置 - 系统会优先使用主要模型(openai)，如果不可用则自动回退到备用模型(ollama)
LLM_PROVIDER=openai  # 可选值: openai, ollama - 设置主要的LLM提供商
LLM_MODEL=Qwen/Qwen2.5-14B-Instruct  # 使用OpenAI提供商时的模型名称
LLM_API_KEY=sk-xxx  # 外部API密钥
LLM_BASE_URL=https://api.siliconflow.cn/v1  # 外部API基础URL
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2048

# 备用Ollama模型配置 - 当主要模型不可用时使用
OLLAMA_MODEL=qwen2.5:3b  # Ollama本地模型名称
OLLAMA_BASE_URL=http://127.0.0.1:11434/v1  # Ollama API基础URL

# Kubernetes配置
K8S_IN_CLUSTER=false
K8S_CONFIG_PATH=./deploy/kubernetes/config
K8S_NAMESPACE=default

# 根因分析配置
RCA_DEFAULT_TIME_RANGE=30
RCA_MAX_TIME_RANGE=1440
RCA_ANOMALY_THRESHOLD=0.65
RCA_CORRELATION_THRESHOLD=0.7

# 预测配置
PREDICTION_MODEL_PATH=data/models/time_qps_auto_scaling_model.pkl
PREDICTION_SCALER_PATH=data/models/time_qps_auto_scaling_scaler.pkl
PREDICTION_MAX_INSTANCES=20
PREDICTION_MIN_INSTANCES=1
PREDICTION_PROMETHEUS_QUERY='rate(nginx_ingress_controller_nginx_process_requests_total{service="ingress-nginx-controller-metrics"}[10m])'

# 通知配置
FEISHU_WEBHOOK=https://open.feishu.cn/open-apis/bot/v2/hook/your-webhook-url
NOTIFICATION_ENABLED=true

# Tavily搜索配置（可选）
TAVILY_API_KEY=
TAVILY_MAX_RESULTS=5

# 小助手配置
RAG_VECTOR_DB_PATH=data/vector_db
RAG_COLLECTION_NAME=aiops-assistant
RAG_KNOWLEDGE_BASE_PATH=data/knowledge_base
RAG_CHUNK_SIZE=1000
RAG_CHUNK_OVERLAP=200
RAG_TOP_K=4
RAG_SIMILARITY_THRESHOLD=0.7
RAG_OPENAI_EMBEDDING_MODEL=BAAI/bge-m3
RAG_OLLAMA_EMBEDDING_MODEL=nomic-embed-text
RAG_MAX_CONTEXT_LENGTH=4000
RAG_TEMPERATURE=0.1