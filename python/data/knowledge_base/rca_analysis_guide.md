# AI-CloudOps æ ¹å› åˆ†æ (RCA) å®Œæ•´æŒ‡å—

## æ¦‚è¿°

AI-CloudOps æ ¹å› åˆ†æ (Root Cause Analysis, RCA) ç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäºäººå·¥æ™ºèƒ½çš„æ™ºèƒ½åˆ†æå¼•æ“ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿå¼‚å¸¸ï¼Œåˆ†ææŒ‡æ ‡ç›¸å…³æ€§ï¼Œå¹¶é€šè¿‡å¤šç§æœºå™¨å­¦ä¹ ç®—æ³•å¿«é€Ÿå®šä½é—®é¢˜æ ¹å› ã€‚

## ğŸ” æ ¸å¿ƒåŠŸèƒ½

### 1. å¤šç»´å¼‚å¸¸æ£€æµ‹
- **Z-Score æ£€æµ‹**: åŸºäºæ ‡å‡†å·®çš„å¼‚å¸¸å€¼æ£€æµ‹
- **IQR æ£€æµ‹**: åŸºäºå››åˆ†ä½æ•°çš„ç¦»ç¾¤å€¼æ£€æµ‹
- **å­¤ç«‹æ£®æ—**: æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ç®—æ³•
- **DBSCAN èšç±»**: å¯†åº¦èšç±»æ£€æµ‹å¼‚å¸¸æ¨¡å¼
- **æ—¶é—´åºåˆ—å¼‚å¸¸**: æ£€æµ‹æ—¶é—´åºåˆ—ä¸­çš„å¼‚å¸¸æ³¢åŠ¨

### 2. æ™ºèƒ½ç›¸å…³æ€§åˆ†æ
- **çš®å°”é€Šç›¸å…³ç³»æ•°**: çº¿æ€§ç›¸å…³æ€§åˆ†æ
- **æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°**: å•è°ƒç›¸å…³æ€§åˆ†æ
- **äº’ä¿¡æ¯**: éçº¿æ€§ç›¸å…³æ€§åº¦é‡
- **æ ¼å…°æ°å› æœæ€§**: æ—¶é—´åºåˆ—å› æœå…³ç³»æ£€éªŒ
- **åŠ¨æ€ç›¸å…³æ€§**: æ—¶é—´çª—å£å†…çš„ç›¸å…³æ€§å˜åŒ–

### 3. æ ¹å› æ¨æ–­
- **å¤šå±‚æ¬¡åˆ†æ**: ä»ç—‡çŠ¶åˆ°æ ¹å› çš„å±‚æ¬¡åŒ–åˆ†æ
- **å› æœé“¾æ„å»º**: æ„å»ºé—®é¢˜ä¼ æ’­è·¯å¾„
- **ç½®ä¿¡åº¦è¯„ä¼°**: ä¸ºæ¯ä¸ªæ ¹å› å‡è®¾æä¾›ç½®ä¿¡åº¦
- **å†å²æ¨¡å¼åŒ¹é…**: ä¸å†å²é—®é¢˜æ¨¡å¼è¿›è¡ŒåŒ¹é…

### 4. æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆ
- **LLM é©±åŠ¨**: åŸºäºå¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆäººç±»å¯è¯»çš„åˆ†ææŠ¥å‘Š
- **å¤šè¯­è¨€æ”¯æŒ**: æ”¯æŒä¸­è‹±æ–‡æŠ¥å‘Šç”Ÿæˆ
- **å¯è§†åŒ–å»ºè®®**: æä¾›å›¾è¡¨å’Œå¯è§†åŒ–å»ºè®®
- **è¡ŒåŠ¨å»ºè®®**: ç”Ÿæˆå…·ä½“çš„ä¿®å¤å»ºè®®

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬ RCA åˆ†æ

```bash
# æ‰§è¡Œæ ¹å› åˆ†æ
curl -X POST http://localhost:8080/api/v1/rca \
  -H "Content-Type: application/json" \
  -d '{
    "start_time": "2024-01-01T10:00:00Z",
    "end_time": "2024-01-01T11:00:00Z",
    "metrics": [
      "container_cpu_usage_seconds_total",
      "container_memory_usage_bytes",
      "http_requests_per_second"
    ]
  }'
```

### 2. é«˜çº§ RCA åˆ†æ

```bash
# å¸¦è‡ªå®šä¹‰å‚æ•°çš„ RCA åˆ†æ
curl -X POST http://localhost:8080/api/v1/rca \
  -H "Content-Type: application/json" \
  -d '{
    "start_time": "2024-01-01T10:00:00Z",
    "end_time": "2024-01-01T11:00:00Z",
    "metrics": ["container_cpu_usage_seconds_total"],
    "anomaly_threshold": 0.7,
    "correlation_threshold": 0.6,
    "include_historical": true,
    "max_candidates": 10
  }'
```

### 3. åˆ†æç»“æœç¤ºä¾‹

```json
{
  "analysis_id": "rca_20240101_120000",
  "timestamp": "2024-01-01T12:00:00Z",
  "time_range": {
    "start": "2024-01-01T10:00:00Z",
    "end": "2024-01-01T11:00:00Z"
  },
  "anomalies": [
    {
      "metric": "container_cpu_usage_seconds_total",
      "severity": "é«˜",
      "score": 0.85,
      "timestamp": "2024-01-01T10:30:00Z",
      "threshold": 0.65,
      "detection_method": "isolation_forest"
    }
  ],
  "correlations": [
    {
      "metric_pair": ["cpu_usage", "response_time"],
      "correlation": 0.87,
      "correlation_type": "pearson",
      "significance": 0.001
    }
  ],
  "root_causes": [
    {
      "description": "CPU ä½¿ç”¨ç‡å¼‚å¸¸å¢é«˜å¯¼è‡´å“åº”æ—¶é—´å»¶é•¿",
      "confidence": 0.92,
      "evidence": [
        "CPU ä½¿ç”¨ç‡åœ¨ 10:30 çªç„¶ä¸Šå‡åˆ° 95%",
        "åŒæ—¶æœŸå“åº”æ—¶é—´å¢åŠ  300%",
        "ä¸¤è€…å­˜åœ¨å¼ºæ­£ç›¸å…³å…³ç³» (r=0.87)"
      ],
      "related_metrics": [
        "container_cpu_usage_seconds_total",
        "http_request_duration_seconds"
      ]
    }
  ],
  "summary": "ç³»ç»Ÿåœ¨ 10:30 å·¦å³å‡ºç° CPU ä½¿ç”¨ç‡å¼‚å¸¸ï¼Œå¯¼è‡´æœåŠ¡å“åº”æ—¶é—´æ˜¾è‘—å¢åŠ ã€‚å»ºè®®æ£€æŸ¥ CPU å¯†é›†å‹ä»»åŠ¡å’Œèµ„æºé…ç½®ã€‚"
}
```

## ğŸ”§ é…ç½®ç®¡ç†

### 1. RCA å¼•æ“é…ç½®

```yaml
rca:
  # å¼‚å¸¸æ£€æµ‹é…ç½®
  anomaly_detection:
    threshold: 0.65
    methods: ["z_score", "iqr", "isolation_forest", "dbscan"]
    min_anomaly_score: 0.7
    
  # ç›¸å…³æ€§åˆ†æé…ç½®
  correlation:
    threshold: 0.6
    methods: ["pearson", "spearman", "mutual_info"]
    significance_level: 0.05
    
  # æ ¹å› åˆ†æé…ç½®
  root_cause:
    max_candidates: 10
    min_confidence: 0.5
    historical_lookback_days: 30
    
  # æŠ¥å‘Šç”Ÿæˆé…ç½®
  reporting:
    language: "zh"
    include_charts: true
    max_summary_length: 1000
```

### 2. Prometheus æŸ¥è¯¢é…ç½®

```yaml
prometheus:
  queries:
    cpu_usage: 'rate(container_cpu_usage_seconds_total[5m])'
    memory_usage: 'container_memory_usage_bytes / 1024 / 1024'
    request_rate: 'rate(http_requests_total[5m])'
    response_time: 'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))'
    error_rate: 'rate(http_requests_total{status=~"5.."}[5m])'
```

### 3. ç®—æ³•å‚æ•°è°ƒä¼˜

```yaml
algorithms:
  z_score:
    threshold: 2.5
    window_size: 100
    
  isolation_forest:
    contamination: 0.1
    n_estimators: 100
    max_samples: "auto"
    
  dbscan:
    eps: 0.5
    min_samples: 5
    metric: "euclidean"
```

## ğŸ“Š å¼‚å¸¸æ£€æµ‹ç®—æ³•è¯¦è§£

### 1. Z-Score æ£€æµ‹

```python
def z_score_detection(data, threshold=2.5):
    """åŸºäº Z-Score çš„å¼‚å¸¸æ£€æµ‹"""
    mean = np.mean(data)
    std = np.std(data)
    z_scores = np.abs((data - mean) / std)
    
    anomalies = []
    for i, score in enumerate(z_scores):
        if score > threshold:
            anomalies.append({
                'index': i,
                'value': data[i],
                'z_score': score,
                'severity': 'high' if score > 3 else 'medium'
            })
    
    return anomalies
```

### 2. å­¤ç«‹æ£®æ—æ£€æµ‹

```python
def isolation_forest_detection(data, contamination=0.1):
    """åŸºäºå­¤ç«‹æ£®æ—çš„å¼‚å¸¸æ£€æµ‹"""
    model = IsolationForest(
        contamination=contamination,
        random_state=42,
        n_estimators=100
    )
    
    # è®­ç»ƒæ¨¡å‹
    model.fit(data.reshape(-1, 1))
    
    # é¢„æµ‹å¼‚å¸¸
    predictions = model.predict(data.reshape(-1, 1))
    scores = model.decision_function(data.reshape(-1, 1))
    
    anomalies = []
    for i, (pred, score) in enumerate(zip(predictions, scores)):
        if pred == -1:  # å¼‚å¸¸ç‚¹
            anomalies.append({
                'index': i,
                'value': data[i],
                'anomaly_score': abs(score),
                'severity': 'high' if abs(score) > 0.5 else 'medium'
            })
    
    return anomalies
```

### 3. DBSCAN èšç±»æ£€æµ‹

```python
def dbscan_anomaly_detection(data, eps=0.5, min_samples=5):
    """åŸºäº DBSCAN çš„å¼‚å¸¸æ£€æµ‹"""
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data.reshape(-1, 1))
    
    # DBSCAN èšç±»
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    clusters = dbscan.fit_predict(scaled_data)
    
    # æ‰¾å‡ºå™ªå£°ç‚¹ï¼ˆå¼‚å¸¸ç‚¹ï¼‰
    anomalies = []
    for i, cluster in enumerate(clusters):
        if cluster == -1:  # å™ªå£°ç‚¹
            anomalies.append({
                'index': i,
                'value': data[i],
                'cluster': cluster,
                'severity': 'medium'
            })
    
    return anomalies
```

## ğŸ”— ç›¸å…³æ€§åˆ†æç®—æ³•

### 1. çš®å°”é€Šç›¸å…³ç³»æ•°

```python
def pearson_correlation(x, y):
    """è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°"""
    correlation, p_value = pearsonr(x, y)
    
    return {
        'correlation': correlation,
        'p_value': p_value,
        'significant': p_value < 0.05,
        'strength': interpret_correlation_strength(abs(correlation))
    }

def interpret_correlation_strength(correlation):
    """è§£é‡Šç›¸å…³æ€§å¼ºåº¦"""
    if correlation >= 0.8:
        return "å¼ºç›¸å…³"
    elif correlation >= 0.6:
        return "ä¸­ç­‰ç›¸å…³"
    elif correlation >= 0.3:
        return "å¼±ç›¸å…³"
    else:
        return "æ— ç›¸å…³"
```

### 2. æ ¼å…°æ°å› æœæ€§æ£€éªŒ

```python
def granger_causality_test(x, y, max_lag=5):
    """æ ¼å…°æ°å› æœæ€§æ£€éªŒ"""
    results = []
    
    for lag in range(1, max_lag + 1):
        try:
            # æ„é€ æ•°æ®
            data = pd.DataFrame({'x': x, 'y': y})
            
            # æ‰§è¡Œæ ¼å…°æ°å› æœæ€§æ£€éªŒ
            result = grangercausalitytests(data[['y', 'x']], maxlag=lag, verbose=False)
            
            # æå– p å€¼
            p_value = result[lag][0]['ssr_ftest'][1]
            
            results.append({
                'lag': lag,
                'p_value': p_value,
                'significant': p_value < 0.05
            })
        except Exception as e:
            logger.warning(f"æ ¼å…°æ°å› æœæ€§æ£€éªŒå¤±è´¥ (lag={lag}): {e}")
    
    return results
```

### 3. äº’ä¿¡æ¯è®¡ç®—

```python
def mutual_information(x, y, n_bins=10):
    """è®¡ç®—äº’ä¿¡æ¯"""
    # ç¦»æ•£åŒ–è¿ç»­å˜é‡
    x_binned = pd.cut(x, bins=n_bins, labels=False)
    y_binned = pd.cut(y, bins=n_bins, labels=False)
    
    # è®¡ç®—äº’ä¿¡æ¯
    mi = mutual_info_score(x_binned, y_binned)
    
    # æ ‡å‡†åŒ–äº’ä¿¡æ¯
    normalized_mi = mi / min(entropy(x_binned), entropy(y_binned))
    
    return {
        'mutual_information': mi,
        'normalized_mi': normalized_mi,
        'strength': interpret_mi_strength(normalized_mi)
    }

def interpret_mi_strength(normalized_mi):
    """è§£é‡Šäº’ä¿¡æ¯å¼ºåº¦"""
    if normalized_mi >= 0.6:
        return "å¼ºä¾èµ–"
    elif normalized_mi >= 0.3:
        return "ä¸­ç­‰ä¾èµ–"
    elif normalized_mi >= 0.1:
        return "å¼±ä¾èµ–"
    else:
        return "æ— ä¾èµ–"
```

## ğŸ§  æ ¹å› æ¨æ–­å¼•æ“

### 1. å› æœé“¾æ„å»º

```python
class CausalChainBuilder:
    """å› æœé“¾æ„å»ºå™¨"""
    
    def __init__(self, correlation_threshold=0.6):
        self.correlation_threshold = correlation_threshold
        self.causal_graph = nx.DiGraph()
    
    def build_causal_chain(self, anomalies, correlations):
        """æ„å»ºå› æœé“¾"""
        # æ·»åŠ å¼‚å¸¸èŠ‚ç‚¹
        for anomaly in anomalies:
            self.causal_graph.add_node(
                anomaly['metric'],
                anomaly_score=anomaly['score'],
                timestamp=anomaly['timestamp']
            )
        
        # æ·»åŠ å› æœè¾¹
        for corr in correlations:
            if corr['correlation'] >= self.correlation_threshold:
                self.causal_graph.add_edge(
                    corr['metric_pair'][0],
                    corr['metric_pair'][1],
                    weight=corr['correlation']
                )
        
        # æ‰¾å‡ºæ ¹å› å€™é€‰
        root_candidates = [
            node for node in self.causal_graph.nodes()
            if self.causal_graph.in_degree(node) == 0
        ]
        
        return root_candidates
```

### 2. ç½®ä¿¡åº¦è¯„ä¼°

```python
def calculate_confidence(evidence):
    """è®¡ç®—æ ¹å› å‡è®¾çš„ç½®ä¿¡åº¦"""
    factors = []
    
    # å¼‚å¸¸ä¸¥é‡ç¨‹åº¦
    severity_score = evidence.get('anomaly_score', 0)
    factors.append(min(severity_score, 1.0))
    
    # ç›¸å…³æ€§å¼ºåº¦
    correlation_score = evidence.get('max_correlation', 0)
    factors.append(abs(correlation_score))
    
    # æ—¶é—´ä¸€è‡´æ€§
    time_consistency = evidence.get('time_consistency', 0)
    factors.append(time_consistency)
    
    # å†å²æ¨¡å¼åŒ¹é…
    historical_match = evidence.get('historical_match', 0)
    factors.append(historical_match)
    
    # åŠ æƒå¹³å‡
    weights = [0.3, 0.3, 0.2, 0.2]
    confidence = sum(f * w for f, w in zip(factors, weights))
    
    return min(confidence, 1.0)
```

### 3. å†å²æ¨¡å¼åŒ¹é…

```python
def match_historical_patterns(current_anomalies, historical_data):
    """åŒ¹é…å†å²é—®é¢˜æ¨¡å¼"""
    matches = []
    
    for historical_case in historical_data:
        similarity = calculate_pattern_similarity(
            current_anomalies,
            historical_case['anomalies']
        )
        
        if similarity > 0.7:
            matches.append({
                'case_id': historical_case['id'],
                'similarity': similarity,
                'root_cause': historical_case['root_cause'],
                'solution': historical_case['solution']
            })
    
    return sorted(matches, key=lambda x: x['similarity'], reverse=True)
```

## ğŸ“‹ æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆ

### 1. LLM é©±åŠ¨çš„åˆ†ææ€»ç»“

```python
async def generate_rca_summary(anomalies, correlations, root_causes):
    """ç”Ÿæˆ RCA åˆ†ææ€»ç»“"""
    
    system_prompt = """
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç³»ç»Ÿè¿ç»´ä¸“å®¶ï¼Œè¯·æ ¹æ®æä¾›çš„å¼‚å¸¸æ£€æµ‹ã€ç›¸å…³æ€§åˆ†æå’Œæ ¹å› åˆ†æç»“æœï¼Œ
    ç”Ÿæˆä¸€ä»½ç®€æ´ã€ä¸“ä¸šçš„æ ¹å› åˆ†ææŠ¥å‘Šã€‚æŠ¥å‘Šåº”åŒ…æ‹¬ï¼š
    1. é—®é¢˜æ¦‚è¿°
    2. ä¸»è¦å‘ç°
    3. æ ¹æœ¬åŸå› 
    4. å½±å“åˆ†æ
    5. å»ºè®®æªæ–½
    """
    
    content = f"""
    ## å¼‚å¸¸æ£€æµ‹ç»“æœ:
    {json.dumps(anomalies, ensure_ascii=False, indent=2)}
    
    ## ç›¸å…³æ€§åˆ†æ:
    {json.dumps(correlations, ensure_ascii=False, indent=2)}
    
    ## æ ¹å› å€™é€‰:
    {json.dumps(root_causes, ensure_ascii=False, indent=2)}
    
    è¯·ç”Ÿæˆè¯¦ç»†çš„æ ¹å› åˆ†ææŠ¥å‘Šã€‚
    """
    
    messages = [{"role": "user", "content": content}]
    
    response = await llm_service.generate_response(
        messages=messages,
        system_prompt=system_prompt,
        temperature=0.3
    )
    
    return response
```

### 2. å¯è§†åŒ–å»ºè®®ç”Ÿæˆ

```python
def generate_visualization_suggestions(analysis_results):
    """ç”Ÿæˆå¯è§†åŒ–å»ºè®®"""
    suggestions = []
    
    # æ—¶é—´åºåˆ—å›¾
    if analysis_results['anomalies']:
        suggestions.append({
            'type': 'time_series',
            'title': 'å¼‚å¸¸æŒ‡æ ‡æ—¶é—´åºåˆ—å›¾',
            'metrics': [a['metric'] for a in analysis_results['anomalies']],
            'description': 'æ˜¾ç¤ºå¼‚å¸¸å‘ç”Ÿçš„æ—¶é—´ç‚¹å’Œä¸¥é‡ç¨‹åº¦'
        })
    
    # ç›¸å…³æ€§çƒ­åŠ›å›¾
    if analysis_results['correlations']:
        suggestions.append({
            'type': 'correlation_heatmap',
            'title': 'æŒ‡æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾',
            'description': 'æ˜¾ç¤ºæŒ‡æ ‡é—´çš„ç›¸å…³æ€§å¼ºåº¦'
        })
    
    # å› æœå…³ç³»å›¾
    if analysis_results['root_causes']:
        suggestions.append({
            'type': 'causal_graph',
            'title': 'å› æœå…³ç³»å›¾',
            'description': 'æ˜¾ç¤ºé—®é¢˜ä¼ æ’­è·¯å¾„å’Œæ ¹å› å…³ç³»'
        })
    
    return suggestions
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. å®æ—¶ RCA ç›‘æ§

```python
class RealTimeRCAMonitor:
    """å®æ—¶ RCA ç›‘æ§å™¨"""
    
    def __init__(self, check_interval=60):
        self.check_interval = check_interval
        self.running = False
    
    async def start_monitoring(self):
        """å¼€å§‹å®æ—¶ç›‘æ§"""
        self.running = True
        
        while self.running:
            try:
                # è·å–æœ€æ–°æŒ‡æ ‡æ•°æ®
                current_time = datetime.now()
                start_time = current_time - timedelta(minutes=10)
                
                metrics_data = await self.collect_metrics(start_time, current_time)
                
                # æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
                anomalies = self.detect_anomalies(metrics_data)
                
                if anomalies:
                    # è§¦å‘ RCA åˆ†æ
                    await self.trigger_rca_analysis(anomalies)
                
                await asyncio.sleep(self.check_interval)
                
            except Exception as e:
                logger.error(f"å®æ—¶ç›‘æ§é”™è¯¯: {e}")
                await asyncio.sleep(self.check_interval)
```

### 2. å¤šç»´åº¦åˆ†æ

```python
def multi_dimensional_analysis(metrics_data, dimensions):
    """å¤šç»´åº¦åˆ†æ"""
    results = {}
    
    for dimension in dimensions:
        # æŒ‰ç»´åº¦åˆ†ç»„æ•°æ®
        grouped_data = group_by_dimension(metrics_data, dimension)
        
        dimension_results = []
        for group_name, group_data in grouped_data.items():
            # å¯¹æ¯ä¸ªç»„è¿›è¡Œ RCA åˆ†æ
            group_anomalies = detect_anomalies(group_data)
            group_correlations = analyze_correlations(group_data)
            
            dimension_results.append({
                'group': group_name,
                'anomalies': group_anomalies,
                'correlations': group_correlations
            })
        
        results[dimension] = dimension_results
    
    return results
```

### 3. é¢„æµ‹æ€§ RCA

```python
def predictive_rca(historical_patterns, current_state):
    """é¢„æµ‹æ€§æ ¹å› åˆ†æ"""
    # åŸºäºå†å²æ¨¡å¼é¢„æµ‹å¯èƒ½çš„é—®é¢˜
    predicted_issues = []
    
    for pattern in historical_patterns:
        # è®¡ç®—å½“å‰çŠ¶æ€ä¸å†å²æ¨¡å¼çš„ç›¸ä¼¼åº¦
        similarity = calculate_state_similarity(current_state, pattern['preconditions'])
        
        if similarity > 0.8:
            predicted_issues.append({
                'issue_type': pattern['issue_type'],
                'probability': similarity,
                'expected_impact': pattern['impact'],
                'prevention_actions': pattern['prevention_actions']
            })
    
    return sorted(predicted_issues, key=lambda x: x['probability'], reverse=True)
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. å¹¶è¡Œå¤„ç†

```python
async def parallel_rca_analysis(metrics_list):
    """å¹¶è¡Œ RCA åˆ†æ"""
    tasks = []
    
    for metrics in metrics_list:
        task = asyncio.create_task(analyze_single_metric(metrics))
        tasks.append(task)
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # åˆå¹¶ç»“æœ
    combined_results = combine_analysis_results(results)
    return combined_results
```

### 2. ç¼“å­˜ä¼˜åŒ–

```python
@lru_cache(maxsize=1000)
def cached_correlation_analysis(metric1_hash, metric2_hash):
    """ç¼“å­˜ç›¸å…³æ€§åˆ†æç»“æœ"""
    # ä»ç¼“å­˜è·å–æˆ–è®¡ç®—ç›¸å…³æ€§
    return calculate_correlation(metric1_hash, metric2_hash)
```

### 3. å¢é‡åˆ†æ

```python
def incremental_rca_analysis(new_data, previous_results):
    """å¢é‡ RCA åˆ†æ"""
    # åªåˆ†ææ–°å¢çš„æ•°æ®ç‚¹
    new_anomalies = detect_new_anomalies(new_data, previous_results)
    
    # æ›´æ–°ç›¸å…³æ€§åˆ†æ
    updated_correlations = update_correlations(new_data, previous_results)
    
    # é‡æ–°è¯„ä¼°æ ¹å› 
    updated_root_causes = reevaluate_root_causes(
        new_anomalies, 
        updated_correlations,
        previous_results
    )
    
    return merge_results(previous_results, {
        'anomalies': new_anomalies,
        'correlations': updated_correlations,
        'root_causes': updated_root_causes
    })
```

## ğŸ” ç›‘æ§å’Œè°ƒè¯•

### 1. RCA æ€§èƒ½æŒ‡æ ‡

```python
# å…³é”®æ€§èƒ½æŒ‡æ ‡
rca_metrics = {
    'analysis_latency': 'åˆ†æå»¶è¿Ÿ',
    'detection_accuracy': 'æ£€æµ‹å‡†ç¡®ç‡',
    'false_positive_rate': 'è¯¯æŠ¥ç‡',
    'root_cause_precision': 'æ ¹å› ç²¾ç¡®åº¦',
    'correlation_computation_time': 'ç›¸å…³æ€§è®¡ç®—æ—¶é—´'
}
```

### 2. è°ƒè¯•å·¥å…·

```bash
# RCA åˆ†æçŠ¶æ€æŸ¥è¯¢
curl -X GET http://localhost:8080/api/v1/rca/status

# å†å²åˆ†æç»“æœæŸ¥è¯¢
curl -X GET http://localhost:8080/api/v1/rca/history?limit=10

# ç®—æ³•æ€§èƒ½åˆ†æ
curl -X GET http://localhost:8080/api/v1/rca/performance
```

### 3. æ—¥å¿—åˆ†æ

```python
# ç»“æ„åŒ– RCA æ—¥å¿—
logger.info("RCA åˆ†æå®Œæˆ", extra={
    'analysis_id': analysis_id,
    'time_range': f"{start_time} - {end_time}",
    'metrics_count': len(metrics),
    'anomalies_found': len(anomalies),
    'processing_time': processing_time,
    'confidence_score': max_confidence
})
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. æ•°æ®è´¨é‡ä¿è¯
- **æ•°æ®æ¸…æ´—**: å»é™¤ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
- **æ—¶é—´å¯¹é½**: ç¡®ä¿ä¸åŒæŒ‡æ ‡çš„æ—¶é—´æˆ³å¯¹é½
- **é‡‡æ ·ç‡ç»Ÿä¸€**: ç»Ÿä¸€ä¸åŒæŒ‡æ ‡çš„é‡‡æ ·é¢‘ç‡

### 2. ç®—æ³•å‚æ•°è°ƒä¼˜
- **é˜ˆå€¼è®¾ç½®**: æ ¹æ®å†å²æ•°æ®è°ƒæ•´å¼‚å¸¸æ£€æµ‹é˜ˆå€¼
- **çª—å£å¤§å°**: åˆç†è®¾ç½®æ—¶é—´çª—å£å¤§å°
- **ç›¸å…³æ€§é˜ˆå€¼**: æ ¹æ®ä¸šåŠ¡ç‰¹ç‚¹è°ƒæ•´ç›¸å…³æ€§é˜ˆå€¼

### 3. ç»“æœéªŒè¯
- **äººå·¥éªŒè¯**: å®šæœŸäººå·¥éªŒè¯ RCA ç»“æœçš„å‡†ç¡®æ€§
- **åé¦ˆå¾ªç¯**: å»ºç«‹åé¦ˆæœºåˆ¶ï¼ŒæŒç»­æ”¹è¿›ç®—æ³•
- **A/B æµ‹è¯•**: å¯¹æ¯”ä¸åŒç®—æ³•çš„æ•ˆæœ

## ğŸš¨ æ•…éšœæ’é™¤

### 1. å¸¸è§é—®é¢˜

#### åˆ†æç»“æœä¸å‡†ç¡®
- **æ£€æŸ¥æ•°æ®è´¨é‡**: ç¡®ä¿è¾“å…¥æ•°æ®çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§
- **è°ƒæ•´å‚æ•°**: æ ¹æ®ä¸šåŠ¡ç‰¹ç‚¹è°ƒæ•´ç®—æ³•å‚æ•°
- **å¢åŠ è®­ç»ƒæ•°æ®**: æ”¶é›†æ›´å¤šå†å²æ•°æ®æé«˜å‡†ç¡®æ€§

#### åˆ†ææ—¶é—´è¿‡é•¿
- **å¯ç”¨å¹¶è¡Œå¤„ç†**: ä½¿ç”¨å¤šçº¿ç¨‹æˆ–å¼‚æ­¥å¤„ç†
- **ä¼˜åŒ–ç®—æ³•**: é€‰æ‹©æ›´é«˜æ•ˆçš„ç®—æ³•å®ç°
- **å¢åŠ ç¼“å­˜**: ç¼“å­˜ä¸­é—´è®¡ç®—ç»“æœ

#### å†…å­˜å ç”¨è¿‡é«˜
- **æ•°æ®åˆ†æ‰¹å¤„ç†**: å°†å¤§æ•°æ®é›†åˆ†æ‰¹å¤„ç†
- **æ¸…ç†ä¸­é—´ç»“æœ**: åŠæ—¶æ¸…ç†ä¸éœ€è¦çš„ä¸­é—´ç»“æœ
- **ä¼˜åŒ–æ•°æ®ç»“æ„**: ä½¿ç”¨æ›´èŠ‚çœå†…å­˜çš„æ•°æ®ç»“æ„

---

*æ ¹å› åˆ†ææ˜¯ AI-CloudOps çš„æ ¸å¿ƒèƒ½åŠ›ä¹‹ä¸€ï¼Œé€šè¿‡æŒç»­ä¼˜åŒ–ç®—æ³•å’Œå¢å¼ºæ™ºèƒ½åŒ–ç¨‹åº¦ï¼Œä¸ºç”¨æˆ·æä¾›æ›´å‡†ç¡®ã€æ›´åŠæ—¶çš„é—®é¢˜è¯Šæ–­èƒ½åŠ›ã€‚*